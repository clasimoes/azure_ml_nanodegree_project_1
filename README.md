# Optimizing an ML Pipeline in Azure

## Overview

This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

The analyzes dataset contains data about the effectiveness of direct marketing campaigns of a banking institution. A campaign is considered successful when a customer accepts to subscribe to the product offered by the bank. We seek to predict if a client will subscribe or not to the product, an answer that can be seen in the column "y" of the provided dataset.

The best performing model to solve this problem was the Logistic Regression from SKLearn that had hyperparameters chosen by HyperDrive. It reached an accuracy of 91.63%. In contrast, the best accuracy reached by AutoML run was reached by a Voting Ensemble model and had a value of 91.58%, which was very close to the best performing model.

## Scikit-learn Pipeline

To solve this problem, a pipeline architecture was created using a notebook in the Microsoft Azure ML Platform. The idea is to use HyperDrive to tune the hyperparameters of a Logistic Regression model and compare its accuracy with many other models from different algorithms using AutoML.

The first step of the pipeline architecture is to create a CPU cluster based on template setting "STANDARD_D2_V2", which is a general-purpose cluster. This cluster is used to run all experiments in this project.

The second step of the pipeline is to perform a HyperDrive run. This run relies on a few things:

. A SKLean estimator, which is a python script that performs the steps of data cleaning and model training. This script uses a Linear Regression model from SKLearn library to fit the training data. There are two hyperparameters that can be passed to the script using a command-line tool, which is the inverse of regularization strength ("C"), and the maximum number of iterations to converge ("max_iter");
. A parameter sampler specified by the user. In this case, we have used Random Parameter Sampling. This is an interesting sampler because it creates many runs based using random choices for the specified parameters using customized probabilistic distributions. I've opted to select parameter "C" from a uniform distribution between 0.001 and 1.0, and to select the "max_iter" parameter between three values: 100, 1000, and 10000;
. A policy to terminate the jobs in the cluster. In this case, I've selected the Bandit Policy, which is based on slack criteria, and a frequency and delay interval for evaluation. If the primary metric (accuracy) is outside the top 10% range of values calculated so far, Azure ML terminates the job and prevents a potential waste of resources;
. A HyperDrive configuration object using all items described before and stating the primary metric that the run intends to optimize ("Accuracy"), how to do it (maximization), and the max number of runs to be sent to the cluster.

With the correct configuration in place, the run can be dispatched. It took about 36 minutes to run 100 different combinations of parameters and this led to the optimal model, which performed 91.63% accuracy.

![HyperDrive Runs and the Best Hyperparameters](hyperdrive_run/best_run_parameters.png)

The final step of the Scikit-learn pipeline was to save the best model in AzureML so that we can allow it to be deployed in production in the future.

![SKLearn HyperDrive Run Summary](hyperdrive_run/run_summary.png)

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The AutoML run to solve the same problem has performed similarly to the best model generated by HyperDrive in terms of accuracy. A "Voting Ensemble" model was the optimal choice in this case.

![AutoML Run Algorithm List](automl_run/best_algorithm.png)

The interesting aspect is that this model is easy explainable using "Explanations" tab. It is possible to see that the feature that has the most impact in the result is the duration of the call. This is intuituvely correct. The second feature that impacts the most is the employment variation rate, which is a quarterly indicator. This also seems quite intuitive because the more people are employed, the more they are likely to pay for a bank service.

![AutoML Algorithm Explanation](automl_run/algorithm_explanation.png)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**



## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
